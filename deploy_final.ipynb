{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw4A5Po7amx8"
   },
   "outputs": [],
   "source": [
    "CWD = \"\"\n",
    "TWITTER_YAML = ''\n",
    "SAVED_TO = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yuepjd8UifX"
   },
   "source": [
    "# Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWai9INjiYBX"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit==1.14.0\n",
    "!pip install --no-cache-dir --upgrade tweepy\n",
    "!pip install gensim\n",
    "!pip install pyngrok\n",
    "!pip install joblib\n",
    "!pip install contractions\n",
    "!pip install pyspellchecker\n",
    "!pip install fasttext\n",
    "!pip install tomotopy\n",
    "!pip install altair\n",
    "!pip install -U scikit-learn\n",
    "!pip install scikeras\n",
    "!pip install tensorflow_text\n",
    "!pip install --upgrade scipy\n",
    "!pip install --upgrade numba "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l23EAl45frbR"
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U80VeaG81sfG",
    "outputId": "8268a1e3-d1b5-4f38-d1bc-193d9492fa39"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "#####################\n",
    "CWD = \"\"\n",
    "TWITTER_YAML = ''\n",
    "SAVED_TO = \"\"\n",
    "\n",
    "#####################\n",
    "import json\n",
    "with open(TWITTER_YAML, 'r') as file:\n",
    "  yamljson = file.read()\n",
    "\n",
    "yamljson = yamljson.replace(\"\\n\", \"\")\n",
    "yamljson = yamljson.replace(\"\\t\", \"\")\n",
    "yamljson = json.loads(yamljson)\n",
    "\n",
    "TWITTER_BEARER_TOKEN = yamljson['twitter']['Bearer_Token']\n",
    "\n",
    "#####################\n",
    "import tweepy\n",
    "import joblib\n",
    "import html\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "import nltk\n",
    "import fasttext \n",
    "import time\n",
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import gensim.corpora.dictionary as Dictionary\n",
    "import gensim.models.tfidfmodel as TfidfModel\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from nltk.collocations import *\n",
    "from nltk.tag import HunposTagger\n",
    "\n",
    "SEPARATOR = \"<new_tweet>\"\n",
    "CHART_WIDTH = 702\n",
    "CHART_HEIGHT = 395\n",
    "PRODUCT_REVIEW = \"product_review_3\"\n",
    "SENTIMENT = \"sentiment4\"\n",
    "DEBUG = False\n",
    "\n",
    "#####################Wrapper function#####################\n",
    "def detect_lang_wrap(tweet):\n",
    "  if not pd.isna(tweet):\n",
    "    return detect_lang.predict(tweet)[0][0]\n",
    "  return np.nan\n",
    "\n",
    "def html_escape_helper(tweet):\n",
    "  if not pd.isna(tweet): #html escape does not accept nan\n",
    "    return html.unescape(tweet)\n",
    "  else:\n",
    "    return tweet\n",
    "\n",
    "def noun_only(x):\n",
    "  # https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2\n",
    "  filtered = [word[0] for word in x if word[1] in [b'NN', 'NN']]\n",
    "  return ' '.join(filtered)  \n",
    "\n",
    "def bigram_filter(bigram):\n",
    "    stop_word_list = stopwords.words('english')\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in stop_word_list or bigram[1] in stop_word_list:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def replace_ngram(x, bigrams):\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(['_'] + gram.split() + ['_']))\n",
    "    return x\n",
    "\n",
    "def return_topic(x, topic):\n",
    "  if not pd.isna(x):\n",
    "    return topic[int(x)]\n",
    "\n",
    "\n",
    "#####################Replace function#####################\n",
    "def replace(series, action, parameter = None):\n",
    "  \n",
    "  #1. standardize\n",
    "  series = series.replace(\"\", np.nan)\n",
    "  series = series.apply(str)\n",
    "  series = series.str.strip()\n",
    "  saved_index = series.index\n",
    "  passage = series.str.cat(sep = \" \" + SEPARATOR + \" \", na_rep = '') #ignore nan + it must \" <new_tweet> \" at least once\n",
    "  passage = re.sub(r\"\\s+\", \" \", passage)\n",
    "  \n",
    "  #2. action\n",
    "  passage = action(passage, parameter) \n",
    "\n",
    "  #3. split back into row\n",
    "  output = pd.Series(passage.split(SEPARATOR))\n",
    "  output = output.str.strip()\n",
    "  output = output.replace(\"nan\", np.nan)\n",
    "  output.index = saved_index\n",
    "  return output\n",
    "\n",
    "def fix_cannot(passage, parameter):\n",
    "  return re.sub(\"cannot\", \"not\", passage)\n",
    "\n",
    "def fix_contractions(passage, parameter):\n",
    "  return contractions.fix(passage)\n",
    "\n",
    "def get_unique_token_from_string(passage, parameter):\n",
    "  return list(Counter(passage.split(\" \")).keys())\n",
    "\n",
    "def stop_word(passage, parameter):  \n",
    "  #1. Build dictionary\n",
    "  vocab = get_unique_token_from_string(passage, parameter)\n",
    "  stop_words = stopwords.words('english') \n",
    "  stop_words.remove(\"no\")\n",
    "  stop_words.remove(\"not\")\n",
    "  stop_words.remove(\"nor\")\n",
    "\n",
    "  #2. Remove\n",
    "  stop_words = Counter(stop_words) #Speed up next line\n",
    "  passage = [word for word in passage.split(\" \") if word not in stop_words] #str.split(\" \") and str.split() are different\n",
    "  passage = ' '.join(passage)\n",
    "  return passage\n",
    "\n",
    "def unknown_word(passage, parameter):\n",
    "  #1. Build dictionary\n",
    "  vocab = get_unique_token_from_string(passage, parameter)\n",
    "  spell = SpellChecker()\n",
    "  misspelled = spell.unknown(vocab)\n",
    "  if \"nan\" in misspelled:\n",
    "    misspelled.remove(\"nan\")\n",
    "  misspelled.remove(SEPARATOR) \n",
    "\n",
    "  #2. Remove\n",
    "  misspelled = Counter(misspelled) #Speed up next line\n",
    "  passage = [word for word in passage.split(\" \") if word not in misspelled] #str.split(\" \") and str.split() are different\n",
    "  passage = ' '.join(passage)\n",
    "  return passage\n",
    "\n",
    "def remove_not(passage, parameter):\n",
    "  remove_word = Counter([\"no\", \"not\", \"nor\"])\n",
    "  passage = [word for word in passage.split(\" \") if word not in remove_word] #str.split(\" \") and str.split() are different\n",
    "  passage = ' '.join(passage)\n",
    "  return passage\n",
    "\n",
    "def reduce_redundancy(passage, parameter):\n",
    "  \n",
    "  #1. Build dictionary\n",
    "  word_counter = Counter(passage.split(\" \"))\n",
    "  word = list(word_counter.keys())\n",
    "\n",
    "  #2. Generating word vector\n",
    "  word_vector_main = word2vec_vector(word)\n",
    "  df_word = {}\n",
    "  for i in range(0, len(word)):\n",
    "    word_vector = word_vector_main[i][0]\n",
    "    if not np.all(word_vector == 0):\n",
    "      df_word[word[i]] = word_vector\n",
    "  df_word = pd.DataFrame.from_dict(df_word, orient = \"index\")\n",
    "\n",
    "  #3. Get clustering of each word\n",
    "  clustering = AgglomerativeClustering(distance_threshold = parameter[\"synonym\"], n_clusters  = None, metric = \"cosine\", linkage = \"complete\").fit(df_word)\n",
    "  df_word = df_word.join(pd.Series(clustering.labels_, index = df_word.index ,name = \"Clusters\"))\n",
    "  df_word = df_word.join(pd.DataFrame.from_dict(word_counter, orient = \"index\", columns = [\"sum_tfidf\"]))\n",
    "\n",
    "  #4. Label cluster with max tfidf\n",
    "  n_clusters = max(df_word.Clusters) + 1\n",
    "  cluster_label = {}\n",
    "\n",
    "  test = df_word.sort_values(by = [\"Clusters\", \"sum_tfidf\"], ascending = [True, False])\n",
    "  test = test.drop_duplicates(subset = \"Clusters\", keep = \"first\")\n",
    "\n",
    "  for i in range(0, len(test[\"Clusters\"])):\n",
    "    cluster_label[test[\"Clusters\"][i]] = test.index[i]\n",
    "\n",
    "  df_word['cluster_label'] = df_word['Clusters'].replace(cluster_label)\n",
    "\n",
    "  df_word = df_word.reset_index()\n",
    "  dictionary = {}\n",
    "  for i in range(0, df_word.shape[0]):\n",
    "    dictionary[df_word.at[i,\"index\"]] = df_word.at[i,\"cluster_label\"]\n",
    "\n",
    "  #5. Replacing word with synonyms\n",
    "  dictionary['<new_tweet>'] = '<new_tweet>'\n",
    "  dictionary[''] = ''\n",
    "  passage =  ' '.join([dictionary[word] for word in passage.split(\" \") if word in dictionary.keys()])\n",
    "\n",
    "  df_word[[\"index\", \"Clusters\", \"sum_tfidf\", 'cluster_label']].to_csv(CWD + \"data/df_word.csv\")\n",
    "  return passage\n",
    "\n",
    "def out_of_vocabulary(passage, parameter):\n",
    "\n",
    "  #1. Built vocabulary\n",
    "  vocab = parameter[\"vocab\"]\n",
    "  word_counter = Counter(passage.split(\" \"))\n",
    "  current_vocab = list(word_counter.keys())\n",
    "  current_vocab.remove(SEPARATOR)\n",
    "  word = current_vocab + list(vocab)\n",
    "\n",
    "  #2. Generating word vector\n",
    "  word_vector_main = word2vec_vector(word)\n",
    "  df_word = {}\n",
    "  for i in range(0, len(word)):\n",
    "    word_vector = word_vector_main[i][0]\n",
    "    if not np.all(word_vector == 0):\n",
    "      df_word[word[i]] = word_vector\n",
    "  df_word = pd.DataFrame.from_dict(df_word, orient = \"index\")\n",
    "\n",
    "  #3. Generating Distance matrix and Get nearest word\n",
    "  dist_mat = pd.DataFrame(cosine_distances(df_word), columns = df_word.index, index = df_word.index)\n",
    "  dist_mat = dist_mat[current_vocab]\n",
    "  dist_mat = dist_mat.loc[vocab]\n",
    "\n",
    "  dist_mat[dist_mat > parameter[\"synonym\"]] = np.nan\n",
    "  dictionary = pd.DataFrame(dist_mat.idxmin(), columns = [\"svm_word\"])\n",
    "  dictionary[\"svm_word\"] = dictionary[\"svm_word\"].replace(np.nan, \"\")\n",
    "  convert = {}\n",
    "  for i in range(0, dictionary.shape[0]):\n",
    "    convert[dictionary.index[i]] = dictionary.iloc[i]['svm_word']\n",
    "\n",
    "  #4. Replacing word with synonyms\n",
    "  convert['<new_tweet>'] = '<new_tweet>'\n",
    "  convert[''] = ''\n",
    "  passage =  ' '.join([convert[word] for word in passage.split(\" \") if word in convert.keys()])\n",
    "  return passage\n",
    "  \n",
    "def stemming(passage, parameter):\n",
    "  #1. Build dictionary\n",
    "  vocab = get_unique_token_from_string(passage, parameter)\n",
    "  term = pd.DataFrame({\"index\": vocab, \"stem\" :vocab})\n",
    "  stemmer = PorterStemmer()\n",
    "  term[\"stem\"] = term[\"stem\"].apply(stemmer.stem)\n",
    "  term[\"len\"] = [len(i) for i in term[\"index\"]]\n",
    "\n",
    "  #2. Repair Stem\n",
    "  test = term.sort_values(by = [\"stem\", \"len\"])\n",
    "  test = test.drop_duplicates(subset = \"stem\", keep = \"first\")\n",
    "  test = test.reset_index(drop = True)\n",
    "  dictionary = {}\n",
    "  for i in range(0, test.shape[0]):\n",
    "    dictionary[test.at[i,\"stem\"]] = test.at[i,\"index\"]\n",
    "  term['stem'] = [dictionary[word] for word in term['stem']]\n",
    "  dictionary = {}\n",
    "  for i in range(0, term.shape[0]):\n",
    "    dictionary[term.at[i,\"index\"]] = term.at[i,\"stem\"]\n",
    "    \n",
    "  #3. Stemming\n",
    "  dictionary['<new_tweet>'] = '<new_tweet>'\n",
    "  dictionary[''] = ''\n",
    "  return ' '.join([dictionary[word] for word in passage.split(\" \") if word in dictionary.keys()])\n",
    "\n",
    "#####################Core function#####################\n",
    "def fetch_tweet(search_query, debug = False):\n",
    "  start = time.time()\n",
    "  if debug:\n",
    "    df = pd.read_csv(CWD + \"data/1.data.csv\")\n",
    "    df = df.rename(columns = {\"tweet\": \"text\"})\n",
    "    df[PRODUCT_REVIEW] = df[PRODUCT_REVIEW].replace({\"Product Review\" : 1, \"Not Product Review\": 0})\n",
    "    df[SENTIMENT] = df[SENTIMENT].replace({\"Positive emotion\" : 2, \"No emotion toward brand or product\": 1, \"Negative emotion\": 0})\n",
    "  else:\n",
    "    query = search_query + \" -is:retweet lang:en\"\n",
    "    client = tweepy.Client(TWITTER_BEARER_TOKEN, return_type = dict)\n",
    "    tweet_dict = tweepy.Paginator(client.search_recent_tweets, query = query, max_results = 100).flatten(limit = 1000)\n",
    "    df = pd.DataFrame([tweet for tweet in tweet_dict])\n",
    "  print(\"0. Fetch Tweet: \", time.time() - start)\n",
    "  return df\n",
    "\n",
    "def text_cleaning(series):\n",
    "\n",
    "  start = time.time()\n",
    "\n",
    "  ##Fix encoding, remove formatting and non-ASCII character\n",
    "  series = series.str.encode(\"utf-8\")\n",
    "  series = series.str.decode(\"utf-8\") \n",
    "  series = series.apply(html_escape_helper)\n",
    "  series = series.str.replace(\"\\n\", \"\", regex = False)\n",
    "  series = series.str.replace(r'[^\\x00-\\x7F]+', \" \", regex = True)\n",
    "  \n",
    "  ##Remove tweet character\n",
    "  series = series.str.replace(r\"RT @\\w+\", \"\", regex = True)\n",
    "  series = series.str.replace(r\"via @\\w+\", \"\", regex = True)\n",
    "  series = series.str.replace(r\"RT\", \"\", regex = False)\n",
    "  series = series.str.replace(r\"@mention\", \"\", regex = False)\n",
    "  series = series.str.replace(r'\\w*\\d\\w*', \"\", regex = True)\n",
    "\n",
    "  ##Remove URL\n",
    "  series = series.str.replace(r\"{link}\", \"\", regex = False)\n",
    "  series = series.str.replace(r\"http\\S+\", \"\", regex = True)\n",
    "  series = series.str.replace(r\"(bit.ly)\\S+\", \"\", regex = True)\n",
    "\n",
    "  print(\"1. Text Cleaning: \", time.time() - start)\n",
    "\n",
    "  return series.copy()\n",
    "\n",
    "def text_preprocessing(series):\n",
    "\n",
    "  table = {'!': ' ', '\"': ' ', '#': '', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '-': '', '.': ' ', '/': ' or ', \n",
    "        ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '_': '', '`': ' ', '{': ' ', '|': ' ', \n",
    "        '}': ' ', '~': ' '}\n",
    "      \n",
    "  start = time.time()\n",
    "  #Lower case\n",
    "  series = series.str.lower()\n",
    "\n",
    "  #Fix contraction\n",
    "  series = series.str.replace(r\"'s\", \"\", regex = False)\n",
    "  series = replace(series, fix_contractions)\n",
    "\n",
    "  #Remove punctuation (This must be the last step)\n",
    "  series = series.str.translate(str.maketrans(table))\n",
    "  \n",
    "  #Remove non English Tweet\n",
    "  temp = pd.concat([series, series.apply(detect_lang_wrap)], axis = 1)\n",
    "  temp.columns = [\"cleaned_tweet\", \"lang\"]\n",
    "  temp.loc[temp['lang'] != \"__label__en\", \"cleaned_tweet\"] = np.nan \n",
    "\n",
    "  #Get noun\n",
    "  temp[\"noun\"] = temp[\"cleaned_tweet\"].replace(\"\", np.nan)\n",
    "  temp[\"noun\"] = temp[\"noun\"].apply(str)\n",
    "  temp[\"noun\"] = temp[\"noun\"].str.strip()\n",
    "  temp[\"noun\"] = temp[\"noun\"].str.replace(r\"\\s+\", \" \", regex = True)\n",
    "  temp[\"noun\"] = temp[\"noun\"].str.split(\" \")\n",
    "  temp[\"noun\"] = temp[\"noun\"].apply(hpt.tag)\n",
    "  temp[\"noun\"] = temp[\"noun\"].apply(noun_only)\n",
    "\n",
    "  #Remove unknown word spelling and stopword \n",
    "  temp['cleaned_tweet'] = replace(temp['cleaned_tweet'], stop_word) \n",
    "  temp['cleaned_tweet'] = replace(temp['cleaned_tweet'], unknown_word)\n",
    "\n",
    "  print(\"2. Text preprocessing: \", time.time() - start)\n",
    "\n",
    "  return temp[\"noun\"], temp['cleaned_tweet']\n",
    "\n",
    "def cleanup(series):\n",
    "  start = time.time()\n",
    "\n",
    "  series = replace(series, fix_cannot) \n",
    "  series = series.str.replace(r\"\\s+\", \" \", regex = True)\n",
    "  series = series.str.strip()\n",
    "\n",
    "  print(\"3. Cleanup: \", time.time() - start)\n",
    "\n",
    "  return series\n",
    "\n",
    "def assign_pr(df, model, vocab, predict_proba = False):\n",
    "\n",
    "  start = time.time()\n",
    "  df['pr_tweet'] = replace(df['cleaned_tweet'], out_of_vocabulary, {\"vocab\": vocab, \"synonym\": 0.38})\n",
    "  \n",
    "  vectorizer = TfidfVectorizer()\n",
    "  X = vectorizer.fit_transform(df[\"pr_tweet\"]).toarray()\n",
    "  X = pd.DataFrame(X, columns = vectorizer.get_feature_names_out())\n",
    "  not_in_X = [i for i in vocab if i not in X.columns]\n",
    "  not_in_X = pd.DataFrame(np.zeros((X.shape[0], len(not_in_X))), columns = [i for i in vocab if i not in X.columns])\n",
    "  X = pd.concat([X, not_in_X], axis = 1 )\n",
    "  X = X.groupby(X.columns, axis = 1).agg(sum)\n",
    "  X = X[vocab]\n",
    "  if predict_proba:\n",
    "    if type(model).__name__ in [\"SVC\"]:\n",
    "      df[PRODUCT_REVIEW] = model.decision_function(X)\n",
    "    elif type(model).__name__ == \"SGDClassifier\":\n",
    "      if model.loss == \"hinge\":\n",
    "        df[PRODUCT_REVIEW] = model.decision_function(X)\n",
    "      else:\n",
    "        df[PRODUCT_REVIEW] = model.predict_proba(X)[:,1]\n",
    "    else:\n",
    "      df[PRODUCT_REVIEW] = model.predict_proba(X)[:,1]\n",
    "  else:\n",
    "      df[PRODUCT_REVIEW] = model.predict(X)\n",
    "  print(\"3. Assign Review: \", time.time() - start)\n",
    "  return df.copy()\n",
    "\n",
    "def assign_sentiment(df, model, predict_proba = False):\n",
    "  start = time.time()\n",
    "  Y = model.predict(df['cleaned_tweet'], verbose = 0)\n",
    "  print(\"3. Assign Sentiment: \", time.time() - start)\n",
    "  if predict_proba:\n",
    "    return Y\n",
    "  else:  \n",
    "    Y = [np.argmax(i) for i in Y]\n",
    "    df[SENTIMENT] = Y\n",
    "    return df.copy()\n",
    "\n",
    "def assign_topic(X, model, test = False):\n",
    "  start = time.time()\n",
    "  #detect bigram\n",
    "  bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "  finder = BigramCollocationFinder.from_documents([str(i).split() for i in X])\n",
    "  finder.apply_freq_filter(7)\n",
    "  if finder.score_ngrams(bigram_measures.pmi) != []: #This mean no ngram found\n",
    "    bigram_list, scores = zip(*finder.score_ngrams(bigram_measures.pmi))\n",
    "    bigram_list = finder.score_ngrams(bigram_measures.pmi)\n",
    "    bigrams = [' '.join(i[0]) for i in bigram_list if bigram_filter(i[0]) if i[1] > 2]\n",
    "    X = X.apply(replace_ngram, args = (bigrams,))\n",
    "  \n",
    "  #model\n",
    "  text_obj = tp.utils.Corpus()\n",
    "  texts = [text_obj.add_doc(str(i).split()) for i in X]\n",
    "  mdl = model(tw = tp.TermWeight.IDF, k = 10, corpus = text_obj, seed = 42)\n",
    "  mdl.burn_in = 100\n",
    "  mdl.train(1000)\n",
    "  \n",
    "  #convert mdl.used_vocab to dict\n",
    "  used_vocabs = {}\n",
    "  for i in range(0, len(mdl.used_vocabs)):\n",
    "    used_vocabs[mdl.used_vocabs[i]] = i\n",
    "\n",
    "  #build topic-term matrix\n",
    "  topic_word_df = pd.DataFrame(columns = mdl.used_vocabs)\n",
    "  for i in range(mdl.k):\n",
    "    topic_word_df = topic_word_df.append(pd.Series(mdl.get_topic_word_dist(topic_id = i), index = mdl.used_vocabs), ignore_index = True)\n",
    "\n",
    "  #tf-idf ranking\n",
    "  vectorizer = TfidfVectorizer(vocabulary = used_vocabs)\n",
    "  vectorizer.fit(X)\n",
    "  idf = pd.Series(vectorizer.idf_, vectorizer.get_feature_names_out())\n",
    "  topic_word_df_out = topic_word_df.mul(idf, axis = 1)\n",
    "\n",
    "  #Labelling\n",
    "  topics_for_coherence = []\n",
    "  topics_for_inference = []\n",
    "  for k in range(mdl.k): \n",
    "    topic_word = topic_word_df_out.iloc[k].copy()\n",
    "    topics_for_coherence.append(list(topic_word[~np.isinf(topic_word)].sort_values(ascending = False).index[0:10]))\n",
    "    topics_for_inference.append(topic_word[~np.isinf(topic_word)].sort_values(ascending = False).index[0])\n",
    "  \n",
    "  #Inference\n",
    "  vectorizer = CountVectorizer(vocabulary = used_vocabs)\n",
    "  noun_count = pd.DataFrame(vectorizer.fit_transform(X).toarray(), columns = vectorizer.get_feature_names_out())\n",
    "  noun_count.index = X.index\n",
    "  noun_count = noun_count.where(noun_count <= 1, 1)\n",
    "  pseudo_prob = np.matmul(noun_count, np.asarray(topic_word_df_out.transpose()))\n",
    "  pseudo_prob = pseudo_prob.div(pseudo_prob.sum(axis = 1), axis = 0)\n",
    "  y = pseudo_prob.idxmax(axis = 1).apply(return_topic, args = (topics_for_inference, ))\n",
    "\n",
    "  #coherence\n",
    "  if test:\n",
    "    texts = [str(i).split() for i in X]\n",
    "    dictionary  = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(i) for i in texts]\n",
    "    model_tfidf = TfidfModel(corpus)\n",
    "    corpus = [model_tfidf[i] for i in corpus]\n",
    "    cm1 = CoherenceModel(topics = topics_for_coherence, texts = texts, corpus = corpus, dictionary = dictionary, coherence='c_v')\n",
    "    print(cm1.get_coherence())\n",
    "  print(\"4. Assign Topic: \", time.time() - start)\n",
    "  return y\n",
    "\n",
    "\n",
    "#####################Main#####################\n",
    "start = time.time()\n",
    "print(\"1/3 Loading Model\")\n",
    "nltk.download('stopwords')\n",
    "print(\"2/3 Loading Model\")\n",
    "nltk.download('punkt')\n",
    "print(\"3/3 Loading Model\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_model():\n",
    "  print(\"1/3 Loading Model\")\n",
    "  detect_lang = fasttext.load_model(CWD + \"model/lid.176.ftz\")\n",
    "  print(\"2/3 Loading Model\")\n",
    "  model_pr = joblib.load(CWD + \"model/model_pr_new.sav\")\n",
    "  print(\"3/3 Loading Model\")\n",
    "  model_st = tf.keras.models.load_model(CWD + 'model/model_st_new.tf')\n",
    "  return detect_lang, model_pr, model_st \n",
    "\n",
    "def word2vec_vector(word):\n",
    "  vectorizer = model_st.get_layer(index = 0)\n",
    "  embedding = model_st.get_layer(index = 1)\n",
    "  return embedding(vectorizer(word)).numpy()\n",
    "\n",
    "detect_lang, model_pr, model_st  = load_model()\n",
    "hpt = HunposTagger(path_to_model = CWD + 'model/hunpos-1.0-linux/english.model', path_to_bin = CWD + 'model/hunpos-1.0-linux/hunpos-tag')\n",
    "print(time.time() - start)\n",
    "\n",
    "st.title(\"Product Sentiment Explorer\")\n",
    "search_query = st.text_input(label = \"Enter a product\")\n",
    "if search_query != '':\n",
    "  start = time.time()\n",
    "  if DEBUG:\n",
    "    search_query = \"iphone\" #Debug\n",
    "  df = fetch_tweet(search_query, debug = False)\n",
    "  df[\"cleaned_tweet\"] = text_cleaning(df[\"text\"])\n",
    "  df[\"noun\"], df[\"cleaned_tweet\"] = text_preprocessing(df[\"cleaned_tweet\"])\n",
    "  df[\"cleaned_tweet\"] = cleanup(df[\"cleaned_tweet\"])\n",
    "  df = df[pd.isna(df['cleaned_tweet']) == False].copy()\n",
    "  df = df[df['cleaned_tweet'] != \"\"].copy()\n",
    "  df = df.drop_duplicates(subset = ['cleaned_tweet'])\n",
    "  df['cleaned_tweet'] = replace(df[\"cleaned_tweet\"], reduce_redundancy, {\"synonym\": 0.38})\n",
    "  df = assign_pr(df, model_pr, vocab = model_pr.feature_names_in_)\n",
    "  df = assign_sentiment(df, model_st)\n",
    "\n",
    "  df['noun'] = replace(df['noun'], stop_word)\n",
    "  df['noun'] = replace(df['noun'], remove_not)\n",
    "  df['noun'] = replace(df['noun'], unknown_word)\n",
    "  df['noun'] = replace(df['noun'], stemming)\n",
    "  df['noun'] = replace(df['noun'], reduce_redundancy, {\"synonym\": 0.20})\n",
    "  df = df[pd.isna(df['noun']) == False].copy()\n",
    "  df = df[df['noun'] != \"\"].copy()\n",
    "  df = df.drop_duplicates(subset = ['noun'])\n",
    "\n",
    "  df_pr = df[df[PRODUCT_REVIEW] == 1].copy()\n",
    "  df_pr[\"topic\"] = assign_topic(df_pr[\"noun\"], tp.PTModel)\n",
    "\n",
    "  df_not_pr = df[df[PRODUCT_REVIEW] == 0].copy()\n",
    "  df_not_pr[\"topic\"] = assign_topic(df_not_pr[\"noun\"], tp.PTModel)\n",
    "  print(\"Total:\", time.time() - start)\n",
    "\n",
    "  st.header(\"Proportion of Product Review Tweet with Positive Sentiment vs Topic\")\n",
    "  df_pr = df_pr[df[SENTIMENT].isin([0,2])][['text','topic', SENTIMENT]].copy()\n",
    "  df_pr[SENTIMENT] = df_pr[SENTIMENT].replace({0:0, 2:1})\n",
    "  chart_data_pr = df_pr.groupby(['topic']).mean()\n",
    "  chart_data_pr = chart_data_pr.sort_values(by = [SENTIMENT], ascending = False)\n",
    "  chart_data_pr = chart_data_pr.reset_index()\n",
    "  st.write(alt.Chart(chart_data_pr).mark_bar().encode(\n",
    "      x = alt.X('topic', sort = None),\n",
    "      y = SENTIMENT\n",
    "  ).properties(\n",
    "    width=CHART_WIDTH,\n",
    "    height=CHART_HEIGHT\n",
    "  ))\n",
    "\n",
    "  st.header(\"Proportion of Non-Product Review Tweet with Positive Sentiment vs Topic\")\n",
    "  df_not_pr = df_not_pr[df[SENTIMENT].isin([0,2])][['text','topic', SENTIMENT]].copy()\n",
    "  df_not_pr[SENTIMENT] = df_not_pr[SENTIMENT].replace({0:0, 2:1})\n",
    "  chart_data_not_pr = df_not_pr.groupby(['topic']).mean()\n",
    "  chart_data_not_pr = chart_data_not_pr.sort_values(by = [SENTIMENT], ascending = False)\n",
    "  chart_data_not_pr = chart_data_not_pr.reset_index()\n",
    "  st.write(alt.Chart(chart_data_not_pr).mark_bar().encode(\n",
    "      x = alt.X('topic', sort = None),\n",
    "      y = SENTIMENT\n",
    "  ).properties(\n",
    "    width=CHART_WIDTH,\n",
    "    height=CHART_HEIGHT\n",
    "  ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGbvDocf1NM"
   },
   "source": [
    "# Run streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqUSuiX42KD6",
    "outputId": "79c4587e-58f2-4f7e-adeb-084df851614b"
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import json\n",
    "\n",
    "with open(TWITTER_YAML, 'r') as file:\n",
    "  yamljson = file.read()\n",
    "\n",
    "yamljson = yamljson.replace(\"\\n\", \"\")\n",
    "yamljson = yamljson.replace(\"\\t\", \"\")\n",
    "yamljson = json.loads(yamljson)\n",
    "\n",
    "NGROK_AUTH_TOKEN = yamljson['ngrok']['NGROK_AUTH_TOKEN']\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "public_url = ngrok.connect(port=8501)\n",
    "print(public_url)\n",
    "!streamlit run --server.port 80 app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEN7ML2wITc3"
   },
   "source": [
    "# Shut down ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTSfIkEsAd2l",
    "outputId": "da2c8576-bcba-4920-e620-32a1b264a7c9"
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "print(ngrok.get_tunnels())\n",
    "ngrok.kill() \n",
    "print(ngrok.get_tunnels())\n",
    "\n",
    "active_tunnels = ngrok.get_tunnels()\n",
    "for tunnel in active_tunnels:\n",
    "  public_url = tunnel.public_url\n",
    "  ngrok.disconnect(public_url)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
