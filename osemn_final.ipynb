{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ--psydWXVB"
   },
   "outputs": [],
   "source": [
    "CWD = \"\"\n",
    "TWITTER_YAML = ''\n",
    "SAVED_TO = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LMkWO3paFB9"
   },
   "source": [
    "# Create word2vec English text file (Run only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5KTwQbdbS6w"
   },
   "source": [
    "## Download Google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIrJvm09XH_t",
    "outputId": "b5ae34ea-e1fc-4065-a611-4402d3ec0fc2"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "word2vec.save_word2vec_format(SAVED_TO + \"model/word2vec.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Yz2MbDbY8V"
   },
   "source": [
    "## Remove non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI5NzzAlYDuj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unicodedata as ud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "latin_letters = {}\n",
    "\n",
    "def is_latin(uchr):\n",
    "  try: \n",
    "    return latin_letters[uchr]\n",
    "  except KeyError:\n",
    "    return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "  return all(is_latin(uchr) for uchr in unistr if uchr.isalpha())\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(SAVED_TO + \"model/word2vec.txt\", \"r\", encoding = \"UTF-8\")\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  if only_roman_chars(word) and spell.unknown([word]) == set():\n",
    "    coefs = np.asarray(values[1:], dtype = \"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()      \n",
    "\n",
    "word_list = list(embeddings_index.keys())\n",
    "f = open(SAVED_TO + \"model/word2vec.en.txt\", \"w\", encoding = \"UTF-8\")\n",
    "\n",
    "for i in range(len(word_list)):\n",
    "  word = word_list[i]\n",
    "  line = [word] + list(embeddings_index[word])\n",
    "  line = [str(i) for i in line]\n",
    "  f.write(' '.join(line) + \"\\n\")\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLx2qdmRb1sX"
   },
   "source": [
    "## Build word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N96pdWdZb7aM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "start = time.time()\n",
    "embeddings_index = {}\n",
    "f = open(SAVED_TO + \"model/word2vec.en.txt\", \"r\", encoding = \"UTF-8\")\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype = \"float32\")\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(vocabulary = list(embeddings_index))\n",
    "word_index = vectorizer.get_vocabulary()\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for i in range(0, len(word_index)):\n",
    "  embedding_vector = embeddings_index.get(word_index[i])\n",
    "  if embedding_vector is not None and len(embedding_vector) == EMBEDDING_DIM:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.TextVectorization(vocabulary = list(embeddings_index.keys())),\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.AUC(from_logits = True, multi_label = True, name = \"auc\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdc9UH1yhuQa"
   },
   "outputs": [],
   "source": [
    "model.save(SAVED_TO + \"model/word2vec_en.tf\", include_optimizer = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYKpB0JQYNEY"
   },
   "source": [
    "# Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6f692maapWZ",
    "outputId": "3ba2d277-81a8-47f5-dde0-99e5d2c41e3f"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit==1.14.0\n",
    "!pip install --no-cache-dir --upgrade tweepy\n",
    "!pip install gensim\n",
    "!pip install pyngrok\n",
    "!pip install joblib\n",
    "!pip install contractions\n",
    "!pip install pyspellchecker\n",
    "!pip install fasttext\n",
    "!pip install tomotopy\n",
    "!pip install altair\n",
    "!pip install -U scikit-learn\n",
    "!pip install scikeras\n",
    "!pip install tensorflow_text\n",
    "!pip install --upgrade scipy\n",
    "!pip install --upgrade numba "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0TOXiNibL8T"
   },
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocRYdt_lbPa3",
    "outputId": "76479aef-1d34-48b4-ea24-eeadb1e6e2d6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "import fasttext\n",
    "import tensorflow as tf\n",
    "from nltk.tag import HunposTagger\n",
    "\n",
    "start = time.time()\n",
    "print(\"1/3 Loading Model\")\n",
    "nltk.download('stopwords')\n",
    "print(\"2/3 Loading Model\")\n",
    "nltk.download('punkt')\n",
    "print(\"3/3 Loading Model\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def load_model():\n",
    "  print(\"1/3 Loading Model\")\n",
    "  detect_lang = fasttext.load_model(CWD + \"model/lid.176.ftz\")\n",
    "  print(\"1/3 Loading Model\")\n",
    "  hpt = HunposTagger(path_to_model = CWD + 'model/hunpos-1.0-linux/english.model', path_to_bin = CWD + 'model/hunpos-1.0-linux/hunpos-tag')\n",
    "  print(\"2/2 Loading Model\")\n",
    "  word2vec = tf.keras.models.load_model(CWD + 'model/word2vec_en.tf')\n",
    "  return detect_lang, hpt, word2vec \n",
    "\n",
    "detect_lang, hpt, word2vec = load_model()\n",
    "\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSlgYcs8bf-I"
   },
   "source": [
    "# Common Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cb-YuJeHazZO",
    "outputId": "3378b84e-9cb5-4dbb-821b-bdeb8d5525f6"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import joblib\n",
    "import html\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "import nltk\n",
    "import fasttext \n",
    "import time\n",
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import gensim.corpora.dictionary as Dictionary\n",
    "import gensim.models.tfidfmodel as TfidfModel\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from nltk.collocations import *\n",
    "\n",
    "SEPARATOR = \"<new_tweet>\"\n",
    "CHART_WIDTH = 702\n",
    "CHART_HEIGHT = 395\n",
    "PRODUCT_REVIEW = \"product_review_3\"\n",
    "SENTIMENT = \"sentiment4\"\n",
    "DEBUG = False\n",
    "\n",
    "#####################Wrapper function#####################\n",
    "def detect_lang_wrap(tweet):\n",
    "  if not pd.isna(tweet):\n",
    "    return detect_lang.predict(tweet)[0][0]\n",
    "  return np.nan\n",
    "\n",
    "def html_escape_helper(tweet):\n",
    "  if not pd.isna(tweet): #html escape does not accept nan\n",
    "    return html.unescape(tweet)\n",
    "  else:\n",
    "    return tweet\n",
    "\n",
    "def noun_only(x):\n",
    "  # https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2\n",
    "  filtered = [word[0] for word in x if word[1] in [b'NN', 'NN']]\n",
    "  return ' '.join(filtered)  \n",
    "\n",
    "def bigram_filter(bigram):\n",
    "    stop_word_list = stopwords.words('english')\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in stop_word_list or bigram[1] in stop_word_list:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def replace_ngram(x, bigrams):\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(['_'] + gram.split() + ['_']))\n",
    "    return x\n",
    "\n",
    "def return_topic(x, topic):\n",
    "  if not pd.isna(x):\n",
    "    return topic[int(x)]\n",
    "\n",
    "\n",
    "#####################Replace function#####################\n",
    "def replace(series, action, parameter = None):\n",
    "  \n",
    "  #1. standardize\n",
    "  series = series.replace(\"\", np.nan)\n",
    "  series = series.apply(str)\n",
    "  series = series.str.strip()\n",
    "  saved_index = series.index\n",
    "  passage = series.str.cat(sep = \" \" + SEPARATOR + \" \", na_rep = '') #ignore nan + it must \" <new_tweet> \" at least once\n",
    "  passage = re.sub(r\"\\s+\", \" \", passage)\n",
    "  \n",
    "  #2. action\n",
    "  passage = action(passage, parameter) \n",
    "\n",
    "  #3. split back into row\n",
    "  output = pd.Series(passage.split(SEPARATOR))\n",
    "  output = output.str.strip()\n",
    "  output = output.replace(\"nan\", np.nan)\n",
    "  output.index = saved_index\n",
    "  return output\n",
    "\n",
    "def fix_cannot(passage, parameter):\n",
    "  return re.sub(\"cannot\", \"not\", passage)\n",
    "\n",
    "def fix_contractions(passage, parameter):\n",
    "  return contractions.fix(passage)\n",
    "\n",
    "def get_unique_token_from_string(passage, parameter):\n",
    "  return list(Counter(passage.split(\" \")).keys())\n",
    "\n",
    "def stop_word(passage, parameter):  \n",
    "  #1. Build dictionary\n",
    "  vocab = get_unique_token_from_string(passage, parameter)\n",
    "  stop_words = stopwords.words('english') \n",
    "  stop_words.remove(\"no\")\n",
    "  stop_words.remove(\"not\")\n",
    "  stop_words.remove(\"nor\")\n",
    "\n",
    "  #2. Remove\n",
    "  stop_words = Counter(stop_words) #Speed up next line\n",
    "  passage = [word for word in passage.split(\" \") if word not in stop_words] #str.split(\" \") and str.split() are different\n",
    "  passage = ' '.join(passage)\n",
    "  return passage\n",
    "\n",
    "def unknown_word(passage, parameter):\n",
    "  #1. Build dictionary\n",
    "  vocab = get_unique_token_from_string(passage, parameter)\n",
    "  spell = SpellChecker()\n",
    "  misspelled = spell.unknown(vocab)\n",
    "  if \"nan\" in misspelled:\n",
    "    misspelled.remove(\"nan\")\n",
    "  misspelled.remove(SEPARATOR) \n",
    "\n",
    "  #2. Remove\n",
    "  misspelled = Counter(misspelled) #Speed up next line\n",
    "  passage = [word for word in passage.split(\" \") if word not in misspelled] #str.split(\" \") and str.split() are different\n",
    "  passage = ' '.join(passage)\n",
    "  return passage\n",
    "\n",
    "def remove_not(passage, parameter):\n",
    "  remove_word = Counter([\"no\", \"not\", \"nor\"])\n",
    "  passage = [word for word in passage.split(\" \") if word not in remove_word] #str.split(\" \") and str.split() are different\n",
    "  passage = ' '.join(passage)\n",
    "  return passage\n",
    "\n",
    "def reduce_redundancy(passage, parameter):\n",
    "  \n",
    "  #1. Build dictionary\n",
    "  word_counter = Counter(passage.split(\" \"))\n",
    "  word = list(word_counter.keys())\n",
    "\n",
    "  #2. Generating word vector\n",
    "  word_vector_main = word2vec_vector(word)\n",
    "  df_word = {}\n",
    "  for i in range(0, len(word)):\n",
    "    word_vector = word_vector_main[i][0]\n",
    "    if not np.all(word_vector == 0):\n",
    "      df_word[word[i]] = word_vector\n",
    "  df_word = pd.DataFrame.from_dict(df_word, orient = \"index\")\n",
    "\n",
    "  #3. Get clustering of each word\n",
    "  clustering = AgglomerativeClustering(distance_threshold = parameter[\"synonym\"], n_clusters  = None, metric = \"cosine\", linkage = \"complete\").fit(df_word)\n",
    "  df_word = df_word.join(pd.Series(clustering.labels_, index = df_word.index ,name = \"Clusters\"))\n",
    "  df_word = df_word.join(pd.DataFrame.from_dict(word_counter, orient = \"index\", columns = [\"sum_tfidf\"]))\n",
    "\n",
    "  #4. Label cluster with max tfidf\n",
    "  n_clusters = max(df_word.Clusters) + 1\n",
    "  cluster_label = {}\n",
    "\n",
    "  test = df_word.sort_values(by = [\"Clusters\", \"sum_tfidf\"], ascending = [True, False])\n",
    "  test = test.drop_duplicates(subset = \"Clusters\", keep = \"first\")\n",
    "\n",
    "  for i in range(0, len(test[\"Clusters\"])):\n",
    "    cluster_label[test[\"Clusters\"][i]] = test.index[i]\n",
    "\n",
    "  df_word['cluster_label'] = df_word['Clusters'].replace(cluster_label)\n",
    "\n",
    "  df_word = df_word.reset_index()\n",
    "  dictionary = {}\n",
    "  for i in range(0, df_word.shape[0]):\n",
    "    dictionary[df_word.at[i,\"index\"]] = df_word.at[i,\"cluster_label\"]\n",
    "\n",
    "  #5. Replacing word with synonyms\n",
    "  dictionary['<new_tweet>'] = '<new_tweet>'\n",
    "  dictionary[''] = ''\n",
    "  passage =  ' '.join([dictionary[word] for word in passage.split(\" \") if word in dictionary.keys()])\n",
    "\n",
    "  df_word[[\"index\", \"Clusters\", \"sum_tfidf\", 'cluster_label']].to_csv(CWD + \"data/df_word.csv\")\n",
    "  return passage\n",
    "\n",
    "def out_of_vocabulary(passage, parameter):\n",
    "\n",
    "  #1. Built vocabulary\n",
    "  vocab = parameter[\"vocab\"]\n",
    "  word_counter = Counter(passage.split(\" \"))\n",
    "  current_vocab = list(word_counter.keys())\n",
    "  current_vocab.remove(SEPARATOR)\n",
    "  word = current_vocab + list(vocab)\n",
    "\n",
    "  #2. Generating word vector\n",
    "  word_vector_main = word2vec_vector(word)\n",
    "  df_word = {}\n",
    "  for i in range(0, len(word)):\n",
    "    word_vector = word_vector_main[i][0]\n",
    "    if not np.all(word_vector == 0):\n",
    "      df_word[word[i]] = word_vector\n",
    "  df_word = pd.DataFrame.from_dict(df_word, orient = \"index\")\n",
    "\n",
    "  #3. Generating Distance matrix and Get nearest word\n",
    "  dist_mat = pd.DataFrame(cosine_distances(df_word), columns = df_word.index, index = df_word.index)\n",
    "  dist_mat = dist_mat[current_vocab]\n",
    "  dist_mat = dist_mat.loc[vocab]\n",
    "\n",
    "  dist_mat[dist_mat > parameter[\"synonym\"]] = np.nan\n",
    "  dictionary = pd.DataFrame(dist_mat.idxmin(), columns = [\"svm_word\"])\n",
    "  dictionary[\"svm_word\"] = dictionary[\"svm_word\"].replace(np.nan, \"\")\n",
    "  convert = {}\n",
    "  for i in range(0, dictionary.shape[0]):\n",
    "    convert[dictionary.index[i]] = dictionary.iloc[i]['svm_word']\n",
    "\n",
    "  #4. Replacing word with synonyms\n",
    "  convert['<new_tweet>'] = '<new_tweet>'\n",
    "  convert[''] = ''\n",
    "  passage =  ' '.join([convert[word] for word in passage.split(\" \") if word in convert.keys()])\n",
    "  return passage\n",
    "  \n",
    "def stemming(passage, parameter):\n",
    "  #1. Build dictionary\n",
    "  vocab = get_unique_token_from_string(passage, parameter)\n",
    "  term = pd.DataFrame({\"index\": vocab, \"stem\" :vocab})\n",
    "  stemmer = PorterStemmer()\n",
    "  term[\"stem\"] = term[\"stem\"].apply(stemmer.stem)\n",
    "  term[\"len\"] = [len(i) for i in term[\"index\"]]\n",
    "\n",
    "  #2. Repair Stem\n",
    "  test = term.sort_values(by = [\"stem\", \"len\"])\n",
    "  test = test.drop_duplicates(subset = \"stem\", keep = \"first\")\n",
    "  test = test.reset_index(drop = True)\n",
    "  dictionary = {}\n",
    "  for i in range(0, test.shape[0]):\n",
    "    dictionary[test.at[i,\"stem\"]] = test.at[i,\"index\"]\n",
    "  term['stem'] = [dictionary[word] for word in term['stem']]\n",
    "  dictionary = {}\n",
    "  for i in range(0, term.shape[0]):\n",
    "    dictionary[term.at[i,\"index\"]] = term.at[i,\"stem\"]\n",
    "    \n",
    "  #3. Stemming\n",
    "  dictionary['<new_tweet>'] = '<new_tweet>'\n",
    "  dictionary[''] = ''\n",
    "  return ' '.join([dictionary[word] for word in passage.split(\" \") if word in dictionary.keys()])\n",
    "\n",
    "#####################Core function#####################\n",
    "def fetch_tweet(search_query, debug = False):\n",
    "  start = time.time()\n",
    "  if debug:\n",
    "    df = pd.read_csv(CWD + \"data/1.data.csv\")\n",
    "    df = df.rename(columns = {\"tweet\": \"text\"})\n",
    "    df[PRODUCT_REVIEW] = df[PRODUCT_REVIEW].replace({\"Product Review\" : 1, \"Not Product Review\": 0})\n",
    "    df[SENTIMENT] = df[SENTIMENT].replace({\"Positive emotion\" : 2, \"No emotion toward brand or product\": 1, \"Negative emotion\": 0})\n",
    "  else:\n",
    "    query = search_query + \" -is:retweet lang:en\"\n",
    "    client = tweepy.Client(BEARER_TOKEN, return_type = dict)\n",
    "    tweet_dict = tweepy.Paginator(client.search_recent_tweets, query = query, max_results = 100).flatten(limit = 1000)\n",
    "    df = pd.DataFrame([tweet for tweet in tweet_dict])\n",
    "  print(\"0. Fetch Tweet: \", time.time() - start)\n",
    "  return df\n",
    "\n",
    "def text_cleaning(series):\n",
    "\n",
    "  start = time.time()\n",
    "\n",
    "  ##Fix encoding, remove formatting and non-ASCII character\n",
    "  series = series.str.encode(\"utf-8\")\n",
    "  series = series.str.decode(\"utf-8\") \n",
    "  series = series.apply(html_escape_helper)\n",
    "  series = series.str.replace(\"\\n\", \"\", regex = False)\n",
    "  series = series.str.replace(r'[^\\x00-\\x7F]+', \" \", regex = True)\n",
    "  \n",
    "  ##Remove tweet character\n",
    "  series = series.str.replace(r\"RT @\\w+\", \"\", regex = True)\n",
    "  series = series.str.replace(r\"via @\\w+\", \"\", regex = True)\n",
    "  series = series.str.replace(r\"RT\", \"\", regex = False)\n",
    "  series = series.str.replace(r\"@mention\", \"\", regex = False)\n",
    "  series = series.str.replace(r'\\w*\\d\\w*', \"\", regex = True)\n",
    "\n",
    "  ##Remove URL\n",
    "  series = series.str.replace(r\"{link}\", \"\", regex = False)\n",
    "  series = series.str.replace(r\"http\\S+\", \"\", regex = True)\n",
    "  series = series.str.replace(r\"(bit.ly)\\S+\", \"\", regex = True)\n",
    "\n",
    "  print(\"1. Text Cleaning: \", time.time() - start)\n",
    "\n",
    "  return series.copy()\n",
    "\n",
    "def text_preprocessing(series):\n",
    "\n",
    "  table = {'!': ' ', '\"': ' ', '#': '', '$': ' ', '%': ' ', '&': ' ', \"'\": ' ', '(': ' ', ')': ' ', '*': ' ', '+': ' ', ',': ' ', '-': '', '.': ' ', '/': ' or ', \n",
    "        ':': ' ', ';': ' ', '<': ' ', '=': ' ', '>': ' ', '?': ' ', '@': ' ', '[': ' ', '\\\\': ' ', ']': ' ', '^': ' ', '_': '', '`': ' ', '{': ' ', '|': ' ', \n",
    "        '}': ' ', '~': ' '}\n",
    "      \n",
    "  start = time.time()\n",
    "  #Lower case\n",
    "  series = series.str.lower()\n",
    "\n",
    "  #Fix contraction\n",
    "  series = series.str.replace(r\"'s\", \"\", regex = False)\n",
    "  series = replace(series, fix_contractions)\n",
    "\n",
    "  #Remove punctuation (This must be the last step)\n",
    "  series = series.str.translate(str.maketrans(table))\n",
    "  \n",
    "  #Remove non English Tweet\n",
    "  temp = pd.concat([series, series.apply(detect_lang_wrap)], axis = 1)\n",
    "  temp.columns = [\"cleaned_tweet\", \"lang\"]\n",
    "  temp.loc[temp['lang'] != \"__label__en\", \"cleaned_tweet\"] = np.nan \n",
    "\n",
    "  #Get noun\n",
    "  temp[\"noun\"] = temp[\"cleaned_tweet\"].replace(\"\", np.nan)\n",
    "  temp[\"noun\"] = temp[\"noun\"].apply(str)\n",
    "  temp[\"noun\"] = temp[\"noun\"].str.strip()\n",
    "  temp[\"noun\"] = temp[\"noun\"].str.replace(r\"\\s+\", \" \", regex = True)\n",
    "  temp[\"noun\"] = temp[\"noun\"].str.split(\" \")\n",
    "  temp[\"noun\"] = temp[\"noun\"].apply(hpt.tag)\n",
    "  temp[\"noun\"] = temp[\"noun\"].apply(noun_only)\n",
    "\n",
    "  #Remove unknown word spelling and stopword \n",
    "  temp['cleaned_tweet'] = replace(temp['cleaned_tweet'], stop_word) \n",
    "  temp['cleaned_tweet'] = replace(temp['cleaned_tweet'], unknown_word)\n",
    "\n",
    "  print(\"2. Text preprocessing: \", time.time() - start)\n",
    "\n",
    "  return temp[\"noun\"], temp['cleaned_tweet']\n",
    "\n",
    "def cleanup(series):\n",
    "  start = time.time()\n",
    "\n",
    "  series = replace(series, fix_cannot) \n",
    "  series = series.str.replace(r\"\\s+\", \" \", regex = True)\n",
    "  series = series.str.strip()\n",
    "\n",
    "  print(\"3. Cleanup: \", time.time() - start)\n",
    "\n",
    "  return series\n",
    "\n",
    "def assign_pr(df, model, vocab, predict_proba = False):\n",
    "\n",
    "  start = time.time()\n",
    "  df['pr_tweet'] = replace(df['cleaned_tweet'], out_of_vocabulary, {\"vocab\": vocab, \"synonym\": 0.38})\n",
    "  \n",
    "  vectorizer = TfidfVectorizer()\n",
    "  X = vectorizer.fit_transform(df[\"pr_tweet\"]).toarray()\n",
    "  X = pd.DataFrame(X, columns = vectorizer.get_feature_names_out())\n",
    "  not_in_X = [i for i in vocab if i not in X.columns]\n",
    "  not_in_X = pd.DataFrame(np.zeros((X.shape[0], len(not_in_X))), columns = [i for i in vocab if i not in X.columns])\n",
    "  X = pd.concat([X, not_in_X], axis = 1 )\n",
    "  X = X.groupby(X.columns, axis = 1).agg(sum)\n",
    "  X = X[vocab]\n",
    "  if predict_proba:\n",
    "    if type(model).__name__ in [\"SVC\"]:\n",
    "      df[PRODUCT_REVIEW] = model.decision_function(X)\n",
    "    elif type(model).__name__ == \"SGDClassifier\":\n",
    "      if model.loss == \"hinge\":\n",
    "        df[PRODUCT_REVIEW] = model.decision_function(X)\n",
    "      else:\n",
    "        df[PRODUCT_REVIEW] = model.predict_proba(X)[:,1]\n",
    "    else:\n",
    "      df[PRODUCT_REVIEW] = model.predict_proba(X)[:,1]\n",
    "  else:\n",
    "      df[PRODUCT_REVIEW] = model.predict(X)\n",
    "  print(\"3. Assign Review: \", time.time() - start)\n",
    "  return df.copy()\n",
    "\n",
    "def assign_sentiment(df, model, predict_proba = False):\n",
    "  start = time.time()\n",
    "  Y = model.predict(df['cleaned_tweet'], verbose = 0)\n",
    "  print(\"3. Assign Sentiment: \", time.time() - start)\n",
    "  if predict_proba:\n",
    "    return Y\n",
    "  else:  \n",
    "    Y = [np.argmax(i) for i in Y]\n",
    "    df[SENTIMENT] = Y\n",
    "    return df.copy()\n",
    "\n",
    "def assign_topic(X, model, test = False):\n",
    "  start = time.time()\n",
    "  #detect bigram\n",
    "  bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "  finder = BigramCollocationFinder.from_documents([str(i).split() for i in X])\n",
    "  finder.apply_freq_filter(7)\n",
    "  if finder.score_ngrams(bigram_measures.pmi) != []: #This mean no ngram found\n",
    "    bigram_list, scores = zip(*finder.score_ngrams(bigram_measures.pmi))\n",
    "    bigram_list = finder.score_ngrams(bigram_measures.pmi)\n",
    "    bigrams = [' '.join(i[0]) for i in bigram_list if bigram_filter(i[0]) if i[1] > 2]\n",
    "    X = X.apply(replace_ngram, args = (bigrams,))\n",
    "  \n",
    "  #model\n",
    "  text_obj = tp.utils.Corpus()\n",
    "  texts = [text_obj.add_doc(str(i).split()) for i in X]\n",
    "  mdl = model(tw = tp.TermWeight.IDF, k = 10, corpus = text_obj, seed = 42)\n",
    "  mdl.burn_in = 100\n",
    "  mdl.train(1000)\n",
    "  \n",
    "  #convert mdl.used_vocab to dict\n",
    "  used_vocabs = {}\n",
    "  for i in range(0, len(mdl.used_vocabs)):\n",
    "    used_vocabs[mdl.used_vocabs[i]] = i\n",
    "\n",
    "  #build topic-term matrix\n",
    "  topic_word_df = pd.DataFrame(columns = mdl.used_vocabs)\n",
    "  for i in range(mdl.k):\n",
    "    topic_word_df = topic_word_df.append(pd.Series(mdl.get_topic_word_dist(topic_id = i), index = mdl.used_vocabs), ignore_index = True)\n",
    "\n",
    "  #tf-idf ranking\n",
    "  vectorizer = TfidfVectorizer(vocabulary = used_vocabs)\n",
    "  vectorizer.fit(X)\n",
    "  idf = pd.Series(vectorizer.idf_, vectorizer.get_feature_names_out())\n",
    "  topic_word_df_out = topic_word_df.mul(idf, axis = 1)\n",
    "\n",
    "  #Labelling\n",
    "  topics_for_coherence = []\n",
    "  topics_for_inference = []\n",
    "  for k in range(mdl.k): \n",
    "    topic_word = topic_word_df_out.iloc[k].copy()\n",
    "    topics_for_coherence.append(list(topic_word[~np.isinf(topic_word)].sort_values(ascending = False).index[0:10]))\n",
    "    topics_for_inference.append(topic_word[~np.isinf(topic_word)].sort_values(ascending = False).index[0])\n",
    "  \n",
    "  #Inference\n",
    "  vectorizer = CountVectorizer(vocabulary = used_vocabs)\n",
    "  noun_count = pd.DataFrame(vectorizer.fit_transform(X).toarray(), columns = vectorizer.get_feature_names_out())\n",
    "  noun_count.index = X.index\n",
    "  noun_count = noun_count.where(noun_count <= 1, 1)\n",
    "  pseudo_prob = np.matmul(noun_count, np.asarray(topic_word_df_out.transpose()))\n",
    "  pseudo_prob = pseudo_prob.div(pseudo_prob.sum(axis = 1), axis = 0)\n",
    "  y = pseudo_prob.idxmax(axis = 1).apply(return_topic, args = (topics_for_inference, ))\n",
    "\n",
    "  #coherence\n",
    "  if test:\n",
    "    texts = [str(i).split() for i in X]\n",
    "    dictionary  = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(i) for i in texts]\n",
    "    model_tfidf = TfidfModel(corpus)\n",
    "    corpus = [model_tfidf[i] for i in corpus]\n",
    "    cm1 = CoherenceModel(topics = topics_for_coherence, texts = texts, corpus = corpus, dictionary = dictionary, coherence='c_v')\n",
    "    print(cm1.get_coherence())\n",
    "  print(\"4. Assign Topic: \", time.time() - start)\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krzs6VC6QDKP"
   },
   "source": [
    "# Obtaining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "cuZRw17CbNIT",
    "outputId": "17b4e8fe-4aa1-4f4f-f053-f02f81d8022c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = fetch_tweet(\"iphone\", debug = True)\n",
    "\n",
    "# https://stackoverflow.com/questions/36004976/count-frequency-of-values-in-pandas-dataframe-column\n",
    "pr = df[PRODUCT_REVIEW].value_counts().to_dict()\n",
    "st = df[SENTIMENT].value_counts().to_dict()\n",
    "\n",
    "# https://www.geeksforgeeks.org/bar-plot-in-matplotlib/\n",
    "plt.figure(figsize = (8, 4.5))\n",
    "plt.bar([\"Not Product Review\", \"Product Review\"], [round(i / sum(pr.values()),2) for i in list(pr.values())])\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (8, 4.5))\n",
    "plt.bar([\"Neutral\", \"Positive\", \"Negative\"], [round(i / sum(st.values()),2) for i in list(st.values())])\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMOD8_p2QHG6"
   },
   "source": [
    "# Scrubbing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugL4_jMFQM9e",
    "outputId": "f832272f-4644-40a2-af75-1241ce9458fb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df[\"cleaned_tweet\"] = text_cleaning(df[\"text\"])\n",
    "df[\"noun\"], df[\"cleaned_tweet\"] = text_preprocessing(df[\"cleaned_tweet\"])\n",
    "df[\"cleaned_tweet\"] = cleanup(df[\"cleaned_tweet\"])\n",
    "df = df[pd.isna(df['cleaned_tweet']) == False].copy()\n",
    "df = df[df['cleaned_tweet'] != \"\"].copy()\n",
    "df = df.drop_duplicates(subset = ['cleaned_tweet'])\n",
    "\n",
    "df_preprocessed_train, df_preprocessed_test = train_test_split(df, test_size=0.20, random_state = 42)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_preprocessed_train.shape)\n",
    "print(df_preprocessed_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVoz7PxlQOzU"
   },
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Yq5TevgRf-x"
   },
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "pSGTw_oQRj0X",
    "outputId": "abc95c69-3d73-4d4a-bd8e-fa5ff3f87224"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# https://www.geeksforgeeks.org/generating-word-cloud-python/#:~:text=For%20generating%20word%20cloud%20in,from%20UCI%20Machine%20Learning%20Repository.\n",
    "comment_words = df_preprocessed_train['cleaned_tweet'].str.cat(sep = \"\", na_rep = '') \n",
    "wordcloud = WordCloud(background_color ='white',stopwords = stop_words).generate(comment_words)\n",
    "\n",
    "plt.figure(figsize = (8, 4.5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I7Eh44qRmb2"
   },
   "source": [
    "## Reduce Redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaD3BxgtpePt"
   },
   "outputs": [],
   "source": [
    "def word2vec_vector(word):\n",
    "  vectorizer = word2vec.get_layer(index = 0)\n",
    "  embedding = word2vec.get_layer(index = 1)\n",
    "  return embedding(vectorizer(word)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4n8ljTnRlzo",
    "outputId": "d45effb7-1f8a-482c-bdfe-71257301c65f"
   },
   "outputs": [],
   "source": [
    "df_train_preprocessed_noredundancy = df_preprocessed_train.copy()\n",
    "df_train_preprocessed_noredundancy['cleaned_tweet'] = replace(df_train_preprocessed_noredundancy['cleaned_tweet'] , reduce_redundancy, {\"synonym\": 0.38})\n",
    "print(\"Before Dimensionality Reduction: \" + str(len(set(df_preprocessed_train['cleaned_tweet'].str.cat(sep = \"\", na_rep = '').split()))))\n",
    "print(\"After Dimensionality Reduction: \" + str(len(set(df_train_preprocessed_noredundancy['cleaned_tweet'].str.cat(sep = \"\", na_rep = '').split()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XOGushQRymw"
   },
   "source": [
    "## Reduce Irrelevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yW1jkRKY404P",
    "outputId": "335e5360-f5b1-489b-daa8-2e242aa94c0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "df_train_preprocessed_noredundancy = df_train_preprocessed_noredundancy[pd.isna(df_train_preprocessed_noredundancy['cleaned_tweet']) == False].copy()\n",
    "df_train_preprocessed_noredundancy = df_train_preprocessed_noredundancy[df_train_preprocessed_noredundancy['cleaned_tweet'] != \"\"].copy()\n",
    "df_train_preprocessed_noredundancy = df_train_preprocessed_noredundancy.drop_duplicates(subset = ['cleaned_tweet'])\n",
    "X = pd.DataFrame(vectorizer.fit_transform(df_train_preprocessed_noredundancy['cleaned_tweet']).toarray(), \n",
    "                                                           columns = vectorizer.get_feature_names_out(), \n",
    "                                                           index = df_train_preprocessed_noredundancy.index)\n",
    "y = df_train_preprocessed_noredundancy[PRODUCT_REVIEW].astype(\"int64\")\n",
    "\n",
    "\n",
    "class_weight = compute_class_weight(class_weight = \"balanced\", classes = np.unique(df_train_preprocessed_noredundancy[PRODUCT_REVIEW]),  y = df_train_preprocessed_noredundancy[PRODUCT_REVIEW])\n",
    "class_weight = {0: class_weight[0], 1: class_weight[1]}\n",
    "clf = ExtraTreesClassifier(criterion = \"entropy\", random_state = 42, class_weight = class_weight)\n",
    "clf = clf.fit(X, y)\n",
    "selector = SelectFromModel(clf, prefit=True)\n",
    "feature = pd.DataFrame(pd.Series(clf.feature_importances_, index = X.columns))\n",
    "feature[\"selected\"] = pd.Series(selector.get_support(), index = X.columns)\n",
    "feature = feature.reset_index()\n",
    "selectedfeature = feature[feature[\"selected\"] == True].sort_values(by = 0, ascending = False)\n",
    "print(\"After Feature Selection: \" + str(selectedfeature.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "yYl_qqlrSoLe",
    "outputId": "944a29e2-cc60-4e29-c5fa-9df2b7906265"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 4.5))\n",
    "plt.bar(selectedfeature[\"index\"][0:10], selectedfeature[0][0:10])\n",
    "plt.ylabel(\"clf.feature_importances_\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6f5LE9cBWPp"
   },
   "outputs": [],
   "source": [
    "np.savetxt(CWD + \"data/feature.csv\", np.reshape(selectedfeature[\"index\"].to_numpy(),(-1, 6)), fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riRS_t7GQi1_"
   },
   "source": [
    "# Modelling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqCyfXQyQ1N-"
   },
   "source": [
    "## Product Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGz4F6YDKswd"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8ANpBAC36FW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = df_train_preprocessed_noredundancy['cleaned_tweet']\n",
    "X = pd.DataFrame(vectorizer.fit_transform(X).toarray(), \n",
    "                 columns = vectorizer.get_feature_names_out(), \n",
    "                 index = X.index)\n",
    "X = X[selectedfeature[\"index\"]]\n",
    "X = X.join(df_train_preprocessed_noredundancy['cleaned_tweet'])\n",
    "y = df_train_preprocessed_noredundancy[PRODUCT_REVIEW].astype(\"int64\")\n",
    "\n",
    "#Train validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state = 42)\n",
    "X_train = X_train.drop('cleaned_tweet', axis = 1)\n",
    "\n",
    "#Set class weight\n",
    "class_weight = compute_class_weight(class_weight = \"balanced\", classes = np.unique(y_train),  y = y_train)\n",
    "class_weight = {0: class_weight[0], 1: class_weight[1]}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wssIDavqKvuO"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8Hy3fxUmH8K"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    NI = len(selectedfeature[\"index\"])\n",
    "    HIDDEN_LAYER_UNIT = round((2 / 3) * NI + 3)\n",
    "    model =tf.keras.Sequential([\n",
    "        tf.keras.Input(shape = (NI, )),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_UNIT, activation='relu'),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_UNIT, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\",loss='binary_crossentropy', weighted_metrics=[tf.keras.metrics.AUC(from_logits=True, name = \"auc\")])\n",
    "    return model\n",
    "\n",
    "model_pr = {\"NB\":BaggingClassifier(ComplementNB()), \n",
    "\"LR\": BaggingClassifier(SGDClassifier(loss = \"log_loss\", class_weight = class_weight, early_stopping = True)), \n",
    "\"DT\": HistGradientBoostingClassifier(loss = \"log_loss\", class_weight = class_weight),\n",
    "\"SVM\": SVC(class_weight = class_weight), \n",
    "\"MLP\": AdaBoostClassifier(KerasClassifier(model=get_model, epochs = 10, \n",
    "                                          validation_split = 0.1, \n",
    "                                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor = \"val_auc\", patience = 1)], \n",
    "                                          class_weight = class_weight), n_estimators = 10, random_state = 42),\n",
    "\"kNN\":BaggingClassifier(KNeighborsClassifier())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFg12YqyK2sI"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykykLj-KV2Ic",
    "outputId": "a7355182-d220-49ab-a608-8e4eece0c4db"
   },
   "outputs": [],
   "source": [
    "metric = {\"NB\": [], \"LR\": [], \"DT\": [], \"SVM\": [], \"MLP\": [], \"kNN\": []}\n",
    "for m in model_pr.keys():\n",
    "  model = model_pr[m]\n",
    "  model.fit(X_train, y_train)\n",
    "  prediction = assign_pr(pd.DataFrame(X_val[\"cleaned_tweet\"]), model, vocab = X_train.columns, predict_proba = True)\n",
    "  acc = roc_auc_score(y_val, prediction[PRODUCT_REVIEW])\n",
    "  metric[m].append(round(acc,4))\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBa8kj6XhCJ9"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kN9y58v7hER5",
    "outputId": "da214c5d-9988-476c-fdaf-531d4d974a0d"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model_pr[\"LR\"], SAVED_TO + \"model/model_pr_new.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0AfFyPySVdk"
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLrbNLkuLrpn"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uufwUwoWSU-y"
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/ldamodel.html \n",
    "df_preprocessed_tp_train, df_preprocessed_tp_validation = train_test_split(df_preprocessed_train, test_size=0.20, random_state = 42)\n",
    "df_preprocessed_tp_train['noun'] = replace(df_preprocessed_tp_train['noun'], stop_word)\n",
    "df_preprocessed_tp_train['noun'] = replace(df_preprocessed_tp_train['noun'], remove_not)\n",
    "df_preprocessed_tp_train['noun'] = replace(df_preprocessed_tp_train['noun'], unknown_word)\n",
    "df_preprocessed_tp_train['noun'] = replace(df_preprocessed_tp_train['noun'], stemming)\n",
    "df_preprocessed_tp_train['noun'] = replace(df_preprocessed_tp_train['noun'], reduce_redundancy, {\"synonym\": 0.20})\n",
    "df_preprocessed_tp_train = df_preprocessed_tp_train[pd.isna(df_preprocessed_tp_train['noun']) == False].copy()\n",
    "df_preprocessed_tp_train = df_preprocessed_tp_train[df_preprocessed_tp_train['noun'] != \"\"].copy()\n",
    "df_preprocessed_tp_train = df_preprocessed_tp_train.drop_duplicates(subset = ['noun'])\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents([str(i).split() for i in df_preprocessed_tp_train['noun']])\n",
    "finder.apply_freq_filter(10)\n",
    "if finder.score_ngrams(bigram_measures.pmi) != []: #This mean no ngram found\n",
    "  bigram_list, scores = zip(*finder.score_ngrams(bigram_measures.pmi))\n",
    "  bigram_list = finder.score_ngrams(bigram_measures.pmi)\n",
    "  bigrams = [' '.join(i[0]) for i in bigram_list if bigram_filter(i[0]) if i[1] > 3]\n",
    "  df_preprocessed_tp_train['noun'] = df_preprocessed_tp_train['noun'].apply(replace_ngram, args = (bigrams,))\n",
    "\n",
    "text_obj = tp.utils.Corpus()\n",
    "texts = [text_obj.add_doc(str(i).split()) for i in df_preprocessed_tp_train['noun']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzOZaBl5L3mH"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnXAbpDIL2c9"
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "mdl1 = tp.LDAModel(tw = tp.TermWeight.IDF, k = 10, corpus = text_obj, seed = 42)\n",
    "\n",
    "#PTM\n",
    "mdl2 = tp.PTModel(tw = tp.TermWeight.IDF, k = 10, corpus = text_obj, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtJdaASLMDrZ"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVO6SV5PMRBY",
    "outputId": "ea2ba895-0807-4328-b999-860aa8e2e338"
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "start = time.time()\n",
    "mdl1.burn_in = 100\n",
    "mdl1.train(1000, workers = 1)\n",
    "print(time.time() - start)\n",
    "\n",
    "#PTM\n",
    "start = time.time()\n",
    "mdl2.burn_in = 100\n",
    "mdl2.train(1000, workers = 1)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGMc8MzEYwgy"
   },
   "outputs": [],
   "source": [
    "def reweigting_topic_term_matrix(model):\n",
    "  #model = mdl1\n",
    "  used_vocabs = {}\n",
    "  for i in range(0, len(model.used_vocabs)):\n",
    "    used_vocabs[model.used_vocabs[i]] = i\n",
    "\n",
    "  #build topic-term matrix\n",
    "  topic_word_df = pd.DataFrame(columns = model.used_vocabs)\n",
    "  for i in range(model.k):\n",
    "    topic_word_df = topic_word_df.append(pd.Series(model.get_topic_word_dist(topic_id = i), index = model.used_vocabs), ignore_index = True)\n",
    "\n",
    "  #tf-idf ranking\n",
    "  vectorizer = TfidfVectorizer(vocabulary = used_vocabs)\n",
    "  vectorizer.fit(df_preprocessed_tp_train['noun'])\n",
    "  idf = pd.Series(vectorizer.idf_, vectorizer.get_feature_names_out())\n",
    "  topic_word_df_out = topic_word_df.mul(idf, axis = 1)\n",
    "\n",
    "  #Labelling\n",
    "  topics = []\n",
    "  for k in range(model.k): \n",
    "    topic_word = topic_word_df_out.iloc[k].copy()\n",
    "    topics.append(list(topic_word[~np.isinf(topic_word)].sort_values(ascending = False).index[0:10]))\n",
    "  return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r99LJUH0NbqF",
    "outputId": "968d731b-8864-46a6-82e7-6cbf0bc861c5"
   },
   "outputs": [],
   "source": [
    "topic_LDA = reweigting_topic_term_matrix(mdl1)\n",
    "topic_PTM = reweigting_topic_term_matrix(mdl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFUpvzJCLis1",
    "outputId": "0e21479f-bf53-499b-a40f-c42ae6958b2f"
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "texts = [str(i).split() for i in df_preprocessed_tp_train['noun']]\n",
    "dictionary  = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(i) for i in texts]\n",
    "model_tfidf = TfidfModel(corpus)\n",
    "corpus = [model_tfidf[i] for i in corpus]\n",
    "\n",
    "cm1 = CoherenceModel(topics = topic_LDA, texts = texts, corpus = corpus, dictionary = dictionary, coherence='c_v')\n",
    "print(cm1.get_coherence())\n",
    "\n",
    "cm2 = CoherenceModel(topics = topic_PTM, texts = texts, corpus = corpus, dictionary = dictionary, coherence='c_v')\n",
    "print(cm2.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YcMuwdIQ5PA"
   },
   "source": [
    "## Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j1nQD0rLC5N"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1KBDRLiRikM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "num_class = 3\n",
    "#Split into train and validation\n",
    "df_train_preprocessed_noredundancy_train, df_train_preprocessed_noredundancy_val = train_test_split(df_train_preprocessed_noredundancy, test_size=0.20, random_state = 42)\n",
    "X_train, y_train = df_train_preprocessed_noredundancy_train[\"cleaned_tweet\"], df_train_preprocessed_noredundancy_train[SENTIMENT]\n",
    "\n",
    "#Prepare validation data\n",
    "X_val, y_val = df_train_preprocessed_noredundancy_val[\"cleaned_tweet\"], df_train_preprocessed_noredundancy_val[SENTIMENT]\n",
    "\n",
    "#Dummy code\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "#To_numpy() tensorflow only want this\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "\n",
    "model_st = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh7eb3lDEQKO",
    "outputId": "f1e813c5-29b6-41ad-fd34-14665c44f1c2"
   },
   "outputs": [],
   "source": [
    "def init_imbalanced_class_weight_bias(df:pd.DataFrame, label:str):\n",
    "  from scipy.optimize import fsolve\n",
    "  from math import exp\n",
    "\n",
    "  # to deal with imbalance classification, calculate class_weight \n",
    "  d = dict(df[label].value_counts())\n",
    "  m = np.mean(list(d.values()))\n",
    "  class_weight = {k:m/v for (k,v) in d.items()} #e.g. {0: 1.6282051282051282, 1: 0.7604790419161677, 2: 0.9338235294117647}\n",
    "  \n",
    "  # define classes frequency list\n",
    "  frequency = list(list(d.values())/sum(d.values()))\n",
    "\n",
    "  # define equations to solve initial bias\n",
    "  def eqn(x, frequency=frequency):\n",
    "      sum_exp = sum([exp(x_i) for x_i in x])\n",
    "      return [exp(x[i])/sum_exp - frequency[i] for i in range(len(frequency))]\n",
    "\n",
    "  # calculate init bias\n",
    "  bias_init = fsolve(func=eqn,\n",
    "                    x0=[0]*len(frequency),\n",
    "                    ).tolist()\n",
    "\n",
    "  return class_weight, bias_init \n",
    "\n",
    "class_weight, bias_init = init_imbalanced_class_weight_bias(df=df_train_preprocessed_noredundancy_val, label=SENTIMENT)\n",
    "bias_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyoBgVdmLHFs"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "un52iCfRRISv",
    "outputId": "929aa967-d71a-427c-94a3-6b6805263d76"
   },
   "outputs": [],
   "source": [
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "#Built Model\n",
    "start = time.time()\n",
    "embeddings_index = {}\n",
    "f = open(CWD + \"model/word2vec.en.txt\", 'r', encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(time.time() - start)\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(vocabulary = list(embeddings_index.keys()))\n",
    "word_index = vectorizer.get_vocabulary()\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for i in range(0, len(word_index)):\n",
    "    embedding_vector = embeddings_index.get(word_index[i])\n",
    "    if embedding_vector is not None and len(embedding_vector) == EMBEDDING_DIM:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "LSTM_UNIT = 128\n",
    "HIDDEN_LAYER_UNIT = round((2 / 3) * LSTM_UNIT + 3)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.TextVectorization(vocabulary = list(embeddings_index.keys())),\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable = False),\n",
    "    tf.keras.layers.LSTM(LSTM_UNIT), \n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_UNIT, activation='relu'),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_UNIT, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_class, activation='softmax', bias_initializer = tf.keras.initializers.Constant(bias_init))\n",
    "])\n",
    "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=[tf.keras.metrics.AUC(from_logits=True, multi_label = True, name = \"auc\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPnJjGM-R2hu",
    "outputId": "ed944ea1-f723-4a86-800f-7d56bb553268"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/sentiment-analysis-using-lstm-and-glove-embeddings-99223a87fe8e\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\", trainable=False)\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/2\", trainable=False)\n",
    "GRU_unit = 128\n",
    "HIDDEN_LAYER_UNIT = round((2 / 3) * GRU_unit + 3)\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2021/12/text-classification-using-bert-and-tensorflow/\n",
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "  preprocessed_text = bert_preprocess_model(text_input)\n",
    "  outputs = bert_encoder(preprocessed_text)\n",
    "  gru = tf.keras.layers.GRU(GRU_unit)(outputs[\"sequence_output\"])\n",
    "  mlp1 = tf.keras.layers.Dense(HIDDEN_LAYER_UNIT, activation='relu')(gru)\n",
    "  mlp2 = tf.keras.layers.Dense(HIDDEN_LAYER_UNIT, activation='relu')(mlp1)\n",
    "  net = tf.keras.layers.Dense(num_class, activation='softmax', bias_initializer = tf.keras.initializers.Constant(bias_init))(mlp2)\n",
    "  return tf.keras.Model(text_input, net)\n",
    "\n",
    "model2 = build_classifier_model()\n",
    "model2.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=[tf.keras.metrics.AUC(from_logits=True, multi_label = True, name = \"auc\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJt5ZuqALYTN"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMa3Yj5MLdb-",
    "outputId": "6bc54c18-48f7-446f-de41-a9d03397d6f8"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=10, callbacks=[tf.keras.callbacks.EarlyStopping(monitor = \"val_auc\", patience = 1)], class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dl1SWnyqLZwq",
    "outputId": "488d5ae2-1c3e-42a4-bfdf-4050758d9a30"
   },
   "outputs": [],
   "source": [
    "model2.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=10, callbacks=[tf.keras.callbacks.EarlyStopping(monitor = \"val_auc\", patience = 1)], class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuHBsGHMP-2j"
   },
   "outputs": [],
   "source": [
    "model_st[\"WE_LSTM\"] = model\n",
    "model_st[\"BERT_GRU\"] = model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwCPFKTOhOGE"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziKHjkvLhQUq",
    "outputId": "575773ed-c884-4c91-c4b4-e92bf190a5cd"
   },
   "outputs": [],
   "source": [
    "model_st[\"WE_LSTM\"].save(SAVED_TO + \"model/model_st_new.tf\", include_optimizer = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b730X6FGQr0S"
   },
   "source": [
    "# Interpreting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "O8-g_i8zj_gL",
    "outputId": "4c661c6f-cf6e-4d00-b561-ad012520a778"
   },
   "outputs": [],
   "source": [
    "##Wait until last minutes\n",
    "df_test = df_preprocessed_test.copy()\n",
    "df_test['cleaned_tweet'] = replace(df_test['cleaned_tweet'], reduce_redundancy, {\"synonym\": 0.20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9CWbwCHkV3p"
   },
   "source": [
    "## Product Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94ZiYLJvQqme",
    "outputId": "b30ad94f-54eb-486f-f798-68834776cb9b"
   },
   "outputs": [],
   "source": [
    "y_val = df_test[PRODUCT_REVIEW].to_numpy()\n",
    "metric_pr = {}\n",
    "vocab = model_pr[\"SVM\"].feature_names_in_\n",
    "for m in model_pr.keys():\n",
    "    prediction = assign_pr(pd.DataFrame(df_test[\"cleaned_tweet\"]), model_pr[m], vocab = vocab, predict_proba = True)\n",
    "    acc = roc_auc_score(y_val, prediction[PRODUCT_REVIEW])\n",
    "    metric_pr[m] = acc\n",
    "metric_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdgv0aXojyfV"
   },
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cqren5YqzSpI",
    "outputId": "3c94dfea-5b02-49ce-efe2-d30a26b9abf6"
   },
   "outputs": [],
   "source": [
    "df_test['noun'] = replace(df_test['noun'], stop_word)\n",
    "df_test['noun'] = replace(df_test['noun'], remove_not)\n",
    "df_test['noun'] = replace(df_test['noun'], unknown_word)\n",
    "df_test['noun'] = replace(df_test['noun'], stemming)\n",
    "df_test['noun'] = replace(df_test['noun'], reduce_redundancy, {\"synonym\": 0.20})\n",
    "df_test = df_test[pd.isna(df_test['noun']) == False].copy()\n",
    "df_test = df_test[df['noun'] != \"\"].copy()\n",
    "df_test = df_test.drop_duplicates(subset = ['noun'])\n",
    "\n",
    "assign_topic(df_test['noun'], tp.LDAModel, test = True)\n",
    "assign_topic(df_test['noun'], tp.PTModel, test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFXDV0WjkmU4"
   },
   "source": [
    "## Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV7uKAickrWF",
    "outputId": "56d58e8e-58e6-4971-8083-fed0a50fceda"
   },
   "outputs": [],
   "source": [
    "y_val = df_test[SENTIMENT].to_numpy()\n",
    "y_val = to_categorical(y_val)\n",
    "metric_st = {}\n",
    "for m in model_st.keys():\n",
    "  prediction = assign_sentiment(pd.DataFrame(df_test[\"cleaned_tweet\"]), model_st[m], predict_proba = True)\n",
    "  acc = roc_auc_score(y_val, prediction, average = \"macro\")\n",
    "  metric_st[m] = acc\n",
    "metric_st  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
